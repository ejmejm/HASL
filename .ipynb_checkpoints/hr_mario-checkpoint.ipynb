{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Breakout Example\n",
    "\n",
    "### Atari Breakout\n",
    "\n",
    "Please do note that this example may take a long time to train.\n",
    "\n",
    "With the default 4 threads runnning on an 8-core CPU with a GTX 1080 Ti, it will take several hours to train to a decent level of play.\n",
    "\n",
    "Running on a platform with more GPU power and a larger cluster of CPUs could siginificantly reduce training time.\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1705.05363.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow.keras.backend import categorical_crossentropy\n",
    "from ludus.policies import BaseTrainer\n",
    "from ludus.env import EnvController\n",
    "from ludus.utils import preprocess_atari, reshape_train_var, discount_rewards\n",
    "from ludus.memory import MTMemoryBuffer\n",
    "import gym\n",
    "import time\n",
    "from mpi4py import MPI\n",
    "import cv2\n",
    "import heapq\n",
    "import multiprocessing as mp\n",
    "# Super Mario stuff\n",
    "from nes_py.wrappers import BinarySpaceToDiscreteSpaceEnv\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import COMPLEX_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "    env = BinarySpaceToDiscreteSpaceEnv(env, COMPLEX_MOVEMENT)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_obs(obs, obs_shape=(42, 42)):\n",
    "    obs = cv2.resize(obs, obs_shape, interpolation=cv2.INTER_LINEAR)\n",
    "    obs = cv2.cvtColor(obs, cv2.COLOR_BGR2GRAY)\n",
    "    return obs / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(action_sets, max_steps=1000):\n",
    "    train_data = []\n",
    "    env = make_env()\n",
    "    obs = env.reset()\n",
    "    obs = filter_obs(obs)\n",
    "    \n",
    "    ep_reward = 0\n",
    "    step = 0\n",
    "    while step < max_steps:\n",
    "        act_idx = np.random.randint(len(action_sets))\n",
    "        act_set = action_sets[act_idx]\n",
    "        \n",
    "        step_reward = 0\n",
    "        for act in act_set:\n",
    "            obs_p, r, d, _ = env.step(act)\n",
    "            step_reward += r\n",
    "            step += 1\n",
    "            if d or step >= max_steps:\n",
    "                break\n",
    "        ep_reward += step_reward\n",
    "        \n",
    "        train_data.append([obs, act_set, step_reward])\n",
    "        \n",
    "        obs_p = filter_obs(obs_p)\n",
    "        train_data[-1].append(obs_p)\n",
    "        obs = obs_p\n",
    "        \n",
    "        if d:\n",
    "            break\n",
    "    \n",
    "    train_data = np.array(train_data)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_act_sets = [[i] for i in range(0, 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data = [worker(train_act_sets, 50) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Reward: 52.93, Min: 19, Max: 93, Std: 10.108664600232812\n"
     ]
    }
   ],
   "source": [
    "reward_list = []\n",
    "for i in range(len(training_data)):\n",
    "    reward_list.append(sum(training_data[i][:,2]))\n",
    "print(f'Avg Reward: {np.mean(reward_list)}, Min: {np.min(reward_list)}, Max: {np.max(reward_list)}, Std: {np.std(reward_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_reward = max(reward_list)\n",
    "scaled_rewards = [r - max_reward for r in reward_list]\n",
    "reward_sum = sum(scaled_rewards)\n",
    "scaled_rewards = [r / reward_sum for r in scaled_rewards]\n",
    "\n",
    "selected_ids = np.random.choice(range(len(training_data)), size=10, replace=False, p=scaled_rewards)\n",
    "top_data = [training_data[idx] for idx in selected_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_branch, max_branch = 2, 3\n",
    "\n",
    "strain_act_sets = set([tuple(x) for x in train_act_sets])\n",
    "branch_dicts = {}\n",
    "for seq_len in range(min_branch, max_branch+1): # For each sequence length\n",
    "    count_dict = {}\n",
    "    for episode in top_data: # Each chosen episode\n",
    "        ep_acts = episode[:,1]\n",
    "        for step_idx in range(seq_len-1, len(ep_acts)):\n",
    "            new_act_set = tuple(np.concatenate(ep_acts[step_idx-seq_len+1:step_idx+1]))\n",
    "            if tuple(new_act_set) not in strain_act_sets:\n",
    "                if new_act_set in count_dict:\n",
    "                    count_dict[new_act_set] += 1\n",
    "                else:\n",
    "                    count_dict[new_act_set] = 1\n",
    "            \n",
    "    branch_dicts[seq_len] = count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 4, 1, 1, 3], [1, 3, 0], [3, 4, 1, 1, 3, 0], [1, 3, 0, 1, 3]]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_top_x = 2\n",
    "top_acts = []\n",
    "for n_branch in range(min_branch, max_branch+1):\n",
    "#     count_list = [c[1] for c in branch_dicts[n_branch].items()]\n",
    "#     max_count = max(count_list)\n",
    "#     scaled_counts = [c - max_reward for r in reward_list]\n",
    "#     reward_sum = sum(scaled_rewards)\n",
    "#     scaled_rewards = [r / reward_sum for r in scaled_rewards]\n",
    "\n",
    "#     selected_ids = np.random.choice(range(len(training_data)), size=10, replace=False, p=scaled_rewards)\n",
    "#     top_data = [training_data[idx] for idx in selected_ids]\n",
    "    \n",
    "    top_acts.extend([list(x[0]) for x in heapq.nlargest(act_top_x, list(branch_dicts[n_branch].items()), key=lambda x: x[1])])\n",
    "    \n",
    "top_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((3, 4, 1, 1, 3, 0), 1),\n",
       " ((1, 3, 0, 1, 3), 1),\n",
       " ((0, 1, 3, 4), 1),\n",
       " ((1, 3, 4, 1), 1),\n",
       " ((4, 1, 0), 1),\n",
       " ((1, 0, 3, 0, 6), 1),\n",
       " ((0, 3, 0, 6, 0), 1),\n",
       " ((3, 0, 6, 0, 0, 3, 4), 1),\n",
       " ((0, 0, 3, 4, 4), 1),\n",
       " ((0, 3, 4, 4, 5), 1),\n",
       " ((4, 5, 3, 3, 0), 1),\n",
       " ((5, 3, 3, 0, 2), 1),\n",
       " ((3, 3, 0, 2, 3, 3, 0), 1),\n",
       " ((2, 3, 3, 0, 3, 4, 1), 1),\n",
       " ((3, 3, 0, 3, 4, 1, 3, 3, 0), 1),\n",
       " ((3, 4, 1, 3, 3, 0, 5), 1),\n",
       " ((3, 3, 0, 5, 5), 1),\n",
       " ((5, 5, 6), 1),\n",
       " ((5, 6, 1, 3), 1),\n",
       " ((6, 1, 3, 3, 4, 1), 1),\n",
       " ((1, 3, 3, 4, 1, 3, 4, 1), 1),\n",
       " ((3, 4, 1, 3, 4, 1, 0, 3, 4), 1),\n",
       " ((3, 4, 1, 0, 3, 4, 3), 1),\n",
       " ((0, 3, 4, 3, 1), 1),\n",
       " ((3, 1, 3, 3), 1)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(branch_dicts[n_branch].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for act in top_acts:\n",
    "    train_act_sets.append(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_episode(action_set, max_steps=50):\n",
    "    env = make_env()\n",
    "    obs = env.reset()\n",
    "    obs = filter_obs(obs)\n",
    "    \n",
    "    ep_reward = 0\n",
    "    for step in range(max_steps):\n",
    "        step_reward = 0\n",
    "        for act in action_set:\n",
    "            env.render()\n",
    "            time.sleep(0.02)\n",
    "            obs_p, r, d, _ = env.step(act)\n",
    "            step_reward += r\n",
    "            if d:\n",
    "                break\n",
    "        ep_reward += step_reward\n",
    "        \n",
    "        obs_p = filter_obs(obs_p)\n",
    "        obs = obs_p\n",
    "        \n",
    "        if d:\n",
    "            break\n",
    "            \n",
    "    print(step, ep_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1525\n"
     ]
    }
   ],
   "source": [
    "render_episode(a[62])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
